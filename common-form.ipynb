{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import keras.backend as K\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nimport re\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report\nfrom os import listdir\nfrom os.path import isfile, join, isdir","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_path = ''\noutput_path = ''\nclassifiers_path = ''\nresults_path = ''\nto_predict_path = ''\ntext_chunk_size = 64\nepochs = 4\nbatch_size = 16\nmax_len = 128\nlearning_rate = 3e-5","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(input_path, output_path, text_chunk_size, epochs, batch_size, max_len, learning_rate):\n    \n    # encoder for encoding the text into sequence of integers for BERT Input\n    def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n\n        tokenizer.enable_truncation(max_length=maxlen)\n        tokenizer.enable_padding(max_length=maxlen)\n        all_ids = []\n\n        for i in tqdm(range(0, len(texts), chunk_size)):\n            text_chunk = texts[i : i + chunk_size].tolist()\n            encodings = tokenizer.encode_batch(text_chunk)\n            all_ids.extend([encoding.ids for encoding in encodings])\n\n        return np.array(all_ids)\n    \n    \n    # function for training the BERT model\n    def build_model(transformer, max_len=512):\n        input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n        sequence_output = transformer(input_word_ids)[0]\n        cls_token = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(cls_token)\n\n        model = Model(inputs=input_word_ids, outputs=out)\n        model.compile(Adam(lr=learning_rate), loss='binary_crossentropy')\n\n        return model\n    \n    \n    # TPU\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n        \n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n\n        strategy = tf.distribute.get_strategy()\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    \n    # batch size according to TPU\n    batch_size *= strategy.num_replicas_in_sync\n    \n    # importing tokenizer\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n    tokenizer.save_pretrained('.')\n    \n    # importing files and creating dataframe\n    files = [file for file in listdir(input_path) if isfile(join(input_path, file))]\n    files.sort()\n    \n    df = pd.DataFrame([])\n    label = 0\n    classes = [file.split('.')[0] for file in files]\n\n    for file in files:\n        file = open(input_path + file, mode='r')\n        label_data = tokenizer.tokenize(file.read())\n        len_label_data = len(label_data)\n        label_df = pd.DataFrame([tokenizer.decode(tokenizer.convert_tokens_to_ids(label_data[i : i + text_chunk_size])) \n                                 for i in range(0, len_label_data, text_chunk_size)])\n        label_df[1] = label\n        df = pd.concat([df, label_df], ignore_index=True)\n        label += 1\n        file.close()\n    \n    # random undersampling\n    min_class_n = df[1].value_counts().min()\n    df = df.groupby(1).apply(lambda x: x.sample(min_class_n)).reset_index(drop=True)\n    \n    # split data\n    X = df[0]\n    y = df[1]\n    x_train_raw, x_test_raw, y_train, y_test = train_test_split(X,\n                                                                y,\n                                                                test_size=0.3,\n                                                                random_state=17)\n    x_valid_raw, x_test_raw, y_valid, y_test = train_test_split(x_test_raw,\n                                                                y_test,\n                                                                test_size=0.33,\n                                                                random_state=17)\n   \n    # import BertWordPieceTokenizer\n    fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n    \n    # dataset encoding\n    x_train_encoded = fast_encode(x_train_raw, fast_tokenizer, maxlen=max_len)\n    x_valid_encoded = fast_encode(x_valid_raw, fast_tokenizer, maxlen=max_len)\n    x_test_encoded = fast_encode(x_test_raw, fast_tokenizer, maxlen=max_len)\n    \n    \n    train_dataset = (tf.data.Dataset\n                       .from_tensor_slices((x_train_encoded, y_train))\n                       .repeat()\n                       .shuffle(17)\n                       .batch(batch_size)\n                       .prefetch(AUTO))\n\n    valid_dataset = (tf.data.Dataset\n                       .from_tensor_slices((x_valid_encoded, y_valid))\n                       .batch(batch_size)\n                       .cache()\n                       .prefetch(AUTO))\n\n    test_dataset = (tf.data.Dataset\n                      .from_tensor_slices(x_test_encoded)\n                      .batch(batch_size))\n    \n    with strategy.scope():\n        transformer_layer = (transformers.TFDistilBertModel\n                                         .from_pretrained('distilbert-base-multilingual-cased'))\n        model = build_model(transformer_layer, max_len)\n    print(model.summary())\n    \n    n_steps = x_train_encoded.shape[0] // batch_size\n\n    train_history = model.fit(train_dataset,\n                              steps_per_epoch=n_steps,\n                              validation_data=valid_dataset,\n                              epochs=epochs)\n    print(train_history)\n    predictions = model.predict(test_dataset).round()\n    \n    # assessments\n    assessments = open(output_path + 'assessments.txt', 'w')\n    assessments.write(classification_report(y_test,\n                                            predictions,\n                                            target_names=classes) + \\\n                                            '\\nConfusion Matrix\\n' + \\\n                      str(tf.math.confusion_matrix(y_test.tolist(),\n                          predictions.round().tolist(),\n                          num_classes=len(classes)).numpy()))\n    assessments.close()\n\n    # save model's hyperparameters\n    parameters = open(output_path + 'model.properties', 'w')\n    parameters.write(f'text_chunk_size={text_chunk_size}\\n' \\\n                     f'epochs={epochs}\\n' \\\n                     f'batch_size={batch_size // strategy.num_replicas_in_sync}\\n' \\\n                     f'max_len={max_len}\\n' \\\n                     f'learning_rate={learning_rate}\\n' \\\n                     f'labels={\",\".join(classes)}')\n    parameters.close()\n    \n    # save model's weights\n    model.save_weights(output_path + 'model_weights.h5')","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(input_path, output_path, text_chunk_size, epochs, batch_size, max_len, learning_rate)","execution_count":54,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 23/23 [00:00<00:00, 35.48it/s]\n100%|██████████| 7/7 [00:00<00:00, 38.51it/s]\n100%|██████████| 4/4 [00:00<00:00, 44.00it/s]\n","name":"stderr"},{"output_type":"stream","text":"Model: \"model_16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 128)]             0         \n_________________________________________________________________\ntf_distil_bert_model_18 (TFD ((None, 128, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice_16 [(None, 768)]             0         \n_________________________________________________________________\ndense_16 (Dense)             (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/4\n45/45 [==============================] - 14s 307ms/step - loss: 0.4312 - val_loss: 0.2782\nEpoch 2/4\n45/45 [==============================] - 4s 91ms/step - loss: 0.1820 - val_loss: 0.1907\nEpoch 3/4\n45/45 [==============================] - 4s 90ms/step - loss: 0.1082 - val_loss: 0.2014\nEpoch 4/4\n45/45 [==============================] - 4s 90ms/step - loss: 0.0705 - val_loss: 0.2573\n<tensorflow.python.keras.callbacks.History object at 0x7f44f899c410>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classification_model(classifiers_path, to_predict_path, results_path):\n    \n    # restore model structure\n    def build_model(transformer, max_len=512):\n        \n        input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n        sequence_output = transformer(input_word_ids)[0]\n        cls_token = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(cls_token)\n\n        model = Model(inputs=input_word_ids, outputs=out)\n        model.compile(Adam(lr=learning_rate), loss='binary_crossentropy')\n        return model\n\n    \n    # encoder for encoding the text into sequence of integers for BERT Input\n    def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n\n        tokenizer.enable_truncation(max_length=maxlen)\n        tokenizer.enable_padding(max_length=maxlen)\n        all_ids = []\n\n        for i in tqdm(range(0, len(texts), chunk_size)):\n            text_chunk = texts[i : i + chunk_size].tolist()\n            encodings = tokenizer.encode_batch(text_chunk)\n            all_ids.extend([encoding.ids for encoding in encodings])\n\n        return np.array(all_ids)\n    \n\n    # TPU\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n        \n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n\n        strategy = tf.distribute.get_strategy()\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n\n    # importing tokenizer\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n    tokenizer.save_pretrained('.')\n\n    # get all directories\n    directories = [directory for directory in listdir(classifiers_path) \n                   if isdir(join(classifiers_path, directory))]\n    \n    # import BertWordPieceTokenizer\n    fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n    \n    # import models\n    with strategy.scope():\n        transformer_layer = (transformers.TFDistilBertModel\n                                         .from_pretrained('distilbert-base-multilingual-cased'))\n\n        classifications = pd.DataFrame([])\n        \n        for directory in directories:\n            parameters = {}\n            with open(classifiers_path + directory + '/model.properties', 'r') as file:\n                lines = file.readlines()\n                for line in lines:\n                    parameter = line.strip('\\n').split('=')\n                    parameters[parameter[0]] = parameter[1]\n            \n            model = build_model(transformer_layer,\n                                int(parameters['max_len']))\n            \n            model.load_weights(classifiers_path + directory + '/model_weights.h5')\n            print(model.summary())\n            \n            # import dataset\n            x_test_raw = pd.read_csv(to_predict_path, sep=';', index_col=0, header=None)\n            x_test_encoded = fast_encode(x_test_raw.iloc[:, 0], fast_tokenizer, maxlen=int(parameters['max_len']))\n            test_dataset = (tf.data.Dataset\n                                   .from_tensor_slices(x_test_encoded)\n                                   .batch(int(parameters['batch_size']) * strategy.num_replicas_in_sync))\n            # predictions\n            predictions = model.predict(test_dataset).round()\n\n            # get labels\n            labels = parameters['labels'].split(',')\n            labels = {index: labels[index] for index in range(len(labels))}\n\n            # add and map predictions\n            classifications = pd.concat([classifications, pd.Series(predictions.ravel()).map(labels)], axis=1)\n    classifications.to_csv(results_path + 'classifications.csv', sep=';', header=None)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_model(classifiers_path, to_predict_path, results_path)","execution_count":56,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1/1 [00:00<00:00, 324.71it/s]","name":"stderr"},{"output_type":"stream","text":"Model: \"model_17\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 128)]             0         \n_________________________________________________________________\ntf_distil_bert_model_19 (TFD ((None, 128, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice_17 [(None, 768)]             0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"},{"output_type":"stream","text":"\n100%|██████████| 1/1 [00:00<00:00, 386.82it/s]","name":"stderr"},{"output_type":"stream","text":"Model: \"model_18\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 128)]             0         \n_________________________________________________________________\ntf_distil_bert_model_19 (TFD ((None, 128, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice_18 [(None, 768)]             0         \n_________________________________________________________________\ndense_18 (Dense)             (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}