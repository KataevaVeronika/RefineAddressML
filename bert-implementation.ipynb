{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","execution_count":109,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Загрузка данных**"},{"metadata":{"trusted":true},"cell_type":"code","source":"negative = pd.read_csv('../input/sentiments/negative.csv', sep=';', header=None)\n# заменяем -1 на 0 в отрицательно окрашенных сообщениях\nnegative[4] = 0\npositive = pd.read_csv('../input/sentiments/positive.csv', sep=';', header=None)\nsentiments = pd.concat([negative, positive]).sample(frac=1).reset_index(drop=True)","execution_count":110,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверяем все ли данные строкового типа в столбце твитов\nany(sentiments[3].map(type) == str)","execution_count":111,"outputs":[{"output_type":"execute_result","execution_count":111,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Находим максимальную длину твита\nmax_str_len = sentiments[3].str.len().max()\nprint(max_str_len)","execution_count":112,"outputs":[{"output_type":"stream","text":"189\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверка на дупликаты\nsentiments.duplicated().any()","execution_count":113,"outputs":[{"output_type":"execute_result","execution_count":113,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверка на missing values\nprint(sentiments[3].isnull().any())\nprint(sentiments[4].isnull().any())","execution_count":114,"outputs":[{"output_type":"stream","text":"False\nFalse\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Делим данные на training, valid, test\nX = sentiments.drop(4, axis=1)\ny = sentiments[4]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.33, random_state=17)","execution_count":115,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Активация TPU**\n\nOn the settings box, bottom-right, select TPU v3-8 and accept the conditions. Execute the next cell, you should see an output message like Running on TPU: grpc://10.0.0.2:8470.\n\nThe code:\n\n1. Initialize the TPU\n2. Instantiate a distribution strategy, this will permit to run the model in parallel on multiple TPU replicas\n3. Return the TPU object containing the distribution strategy settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":116,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# hyperparameters\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = max_str_len\nLEARNING_RATE = 3e-5","execution_count":117,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Загрузка BertWordPieceTokenizer-а\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\ntokenizer.save_pretrained('.')\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","execution_count":118,"outputs":[{"output_type":"execute_result","execution_count":118,"data":{"text/plain":"Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=False, wordpieces_prefix=##)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i : i + chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = fast_encode(X_train[3], fast_tokenizer, maxlen=MAX_LEN)\nx_valid = fast_encode(X_valid[3], fast_tokenizer, maxlen=MAX_LEN)\nx_test = fast_encode(X_test[3], fast_tokenizer, maxlen=MAX_LEN)","execution_count":120,"outputs":[{"output_type":"stream","text":"100%|██████████| 621/621 [00:13<00:00, 46.67it/s]\n100%|██████████| 179/179 [00:03<00:00, 48.74it/s]\n100%|██████████| 88/88 [00:01<00:00, 48.71it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (tf.data.Dataset\n                   .from_tensor_slices((x_train, y_train))\n                   .repeat()\n                   .shuffle(17)\n                   .batch(BATCH_SIZE)\n                   .prefetch(AUTO))\n\nvalid_dataset = (tf.data.Dataset\n                   .from_tensor_slices((x_valid, y_valid))\n                   .batch(BATCH_SIZE)\n                   .cache()\n                   .prefetch(AUTO))\n\ntest_dataset = (tf.data.Dataset\n                  .from_tensor_slices(x_test)\n                  .batch(BATCH_SIZE))","execution_count":121,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# F1-score\ndef f1_score(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    return 2 * precision * recall / (precision + recall + K.epsilon())","execution_count":122,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    Function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy',\n                                                                               Precision(),\n                                                                               Recall(),\n                                                                               f1_score])\n    \n    return model","execution_count":123,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Обучение модели**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (transformers.TFDistilBertModel\n                                     .from_pretrained('distilbert-base-multilingual-cased'))\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","execution_count":124,"outputs":[{"output_type":"stream","text":"Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 189)]             0         \n_________________________________________________________________\ntf_distil_bert_model_2 (TFDi ((None, 189, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice_2  [(None, 768)]             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 9.08 s, sys: 6.97 s, total: 16 s\nWall time: 17.1 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\n\ntrain_history = model.fit(train_dataset,\n                          steps_per_epoch=n_steps,\n                          validation_data=valid_dataset,\n                          epochs=EPOCHS)","execution_count":125,"outputs":[{"output_type":"stream","text":"1240/1240 [==============================] - 149s 120ms/step - f1_score: 0.9920 - accuracy: 0.9934 - loss: 0.0138 - precision_2: 0.9937 - recall_2: 0.9932 - val_f1_score: 0.9995 - val_accuracy: 0.9999 - val_loss: 4.8078e-04 - val_precision_2: 1.0000 - val_recall_2: 0.9997\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = X_valid.shape[0] // BATCH_SIZE\ntrain_history_2 = model.fit(valid_dataset.repeat(),\n                            steps_per_epoch=n_steps,\n                            epochs=EPOCHS*2)","execution_count":126,"outputs":[{"output_type":"stream","text":"Epoch 1/2\n356/356 [==============================] - 36s 100ms/step - f1_score: 0.9995 - accuracy: 0.9996 - loss: 0.0015 - precision_2: 0.9996 - recall_2: 0.9996\nEpoch 2/2\n356/356 [==============================] - 36s 102ms/step - f1_score: 0.9996 - accuracy: 1.0000 - loss: 1.0175e-04 - precision_2: 1.0000 - recall_2: 1.0000\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Confusion matrix на тестовых данных**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(test_dataset)\ntf.math.confusion_matrix(y_test.tolist(),\n                         y_pred.round().tolist(),\n                         num_classes=2)","execution_count":152,"outputs":[{"output_type":"stream","text":"tf.Tensor(\n[[10951     5]\n [    1 11500]], shape=(2, 2), dtype=int32)\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}