{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"**Загрузка данных**"},{"metadata":{"trusted":true},"cell_type":"code","source":"negative = pd.read_csv('../input/sentiments/negative.csv', sep=';', header=None)\n# заменяем -1 на 0 в отрицательно окрашенных сообщениях\nnegative[4] = 0\npositive = pd.read_csv('../input/sentiments/positive.csv', sep=';', header=None)\nsentiments = pd.concat([negative, positive]).sample(frac=1).reset_index(drop=True)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверяем все ли данные строкового типа в столбце твитов\nany(sentiments[3].map(type) == str)","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Находим максимальную длину твита\nmax_str_len = sentiments[3].str.len().max()\nprint(max_str_len)","execution_count":4,"outputs":[{"output_type":"stream","text":"189\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверка на дупликаты\nsentiments.duplicated().any()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверка на missing values\nprint(sentiments[3].isnull().any())\nprint(sentiments[4].isnull().any())","execution_count":6,"outputs":[{"output_type":"stream","text":"False\nFalse\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Делим данные на training, valid, test\nX = sentiments.drop(4, axis=1)\ny = sentiments[4]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Активация TPU**\n\nOn the settings box, bottom-right, select TPU v3-8 and accept the conditions. Execute the next cell, you should see an output message like Running on TPU: grpc://10.0.0.2:8470.\n\nThe code:\n\n1. Initialize the TPU\n2. Instantiate a distribution strategy, this will permit to run the model in parallel on multiple TPU replicas\n3. Return the TPU object containing the distribution strategy settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":8,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# hyperparameters\nEPOCHS = 3\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 256\nLEARNING_RATE = 1e-5","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Загрузка BertWordPieceTokenizer-а\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\ntokenizer.save_pretrained('.')\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"286beda602544da68a5b4309169b80f6"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=False, wordpieces_prefix=##)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i : i + chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = fast_encode(X_train[3], fast_tokenizer, maxlen=MAX_LEN)\nx_test = fast_encode(X_test[3], fast_tokenizer, maxlen=MAX_LEN)","execution_count":12,"outputs":[{"output_type":"stream","text":"100%|██████████| 621/621 [00:16<00:00, 37.46it/s]\n100%|██████████| 266/266 [00:06<00:00, 43.51it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (tf.data.Dataset\n                   .from_tensor_slices((x_train, y_train))\n                   .repeat()\n                   .shuffle(17)\n                   .batch(BATCH_SIZE)\n                   .prefetch(AUTO))\n\ntest_dataset = (tf.data.Dataset\n                  .from_tensor_slices((x_test, y_test))\n                  .batch(BATCH_SIZE)\n                  .cache()\n                  .prefetch(AUTO))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    Function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Обучение модели**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (transformers.TFDistilBertModel\n                                     .from_pretrained('distilbert-base-multilingual-cased'))\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c938b1c33a747ddb6150ef6e1fe9cf9"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=910749124.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4507cbb72f84ac69a34dd1c13de4370"}},"metadata":{}},{"output_type":"stream","text":"\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 256)]             0         \n_________________________________________________________________\ntf_distil_bert_model (TFDist ((None, 256, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice (T [(None, 768)]             0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 35.8 s, sys: 13.6 s, total: 49.4 s\nWall time: 1min 3s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\n\ntrain_history = model.fit(train_dataset,\n                          steps_per_epoch=n_steps,\n                          validation_data=test_dataset,\n                          epochs=EPOCHS)","execution_count":16,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n1240/1240 [==============================] - 161s 130ms/step - loss: 0.0115 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9997\nEpoch 2/3\n1240/1240 [==============================] - 149s 120ms/step - loss: 7.1309e-04 - accuracy: 0.9998 - val_loss: 6.8362e-04 - val_accuracy: 0.9998\nEpoch 3/3\n1240/1240 [==============================] - 150s 121ms/step - loss: 4.9639e-04 - accuracy: 0.9999 - val_loss: 8.9564e-05 - val_accuracy: 1.0000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = X_test.shape[0] // BATCH_SIZE\n\ntrain_history_2 = model.fit(test_dataset.repeat(),\n                            steps_per_epoch=n_steps,\n                            epochs=EPOCHS*2)","execution_count":17,"outputs":[{"output_type":"stream","text":"Epoch 1/6\n531/531 [==============================] - 56s 105ms/step - loss: 5.1917e-04 - accuracy: 0.9999\nEpoch 2/6\n531/531 [==============================] - 56s 106ms/step - loss: 4.6085e-04 - accuracy: 0.9999\nEpoch 3/6\n531/531 [==============================] - 56s 105ms/step - loss: 3.8021e-05 - accuracy: 1.0000\nEpoch 4/6\n531/531 [==============================] - 56s 105ms/step - loss: 4.7776e-06 - accuracy: 1.0000\nEpoch 5/6\n531/531 [==============================] - 56s 106ms/step - loss: 2.0826e-06 - accuracy: 1.0000\nEpoch 6/6\n531/531 [==============================] - 56s 105ms/step - loss: 9.4655e-07 - accuracy: 1.0000\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}