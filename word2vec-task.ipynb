{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install wget\n!pip install ufal.udpipe","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting wget\n  Downloading wget-3.2.zip (10 kB)\nBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9681 sha256=0c57b75168b3fcb9a900ee75d3931c32b7e2183a89cbf3e8f53004170083e458\n  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\nCollecting ufal.udpipe\n  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n\u001b[K     |████████████████████████████████| 304 kB 403 kB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: ufal.udpipe\n  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5742291 sha256=0d31c66ae5b2543bdf027df87d224e75e183f1827552fce448ceffd4df679004\n  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\nSuccessfully built ufal.udpipe\nInstalling collected packages: ufal.udpipe\nSuccessfully installed ufal.udpipe-1.2.0.3\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gensim\nimport numpy as np\nimport os\nimport re\nimport sys\nimport wget\nimport zipfile\nfrom ufal.udpipe import Model, Pipeline","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Функции преобразования текста"},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_replace(word):\n    newtoken = 'x' * len(word)\n    return newtoken\n\n\ndef clean_token(token, misc):\n    out_token = token.strip().replace(' ', '')\n    if token == 'Файл' and 'SpaceAfter=No' in misc:\n        return None\n    return out_token\n\n\ndef clean_lemma(lemma, pos):\n    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n        return None\n    if pos != 'PUNCT':\n        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n            out_lemma = ''.join(out_lemma[1:])\n        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n            out_lemma = ''.join(out_lemma[:-1])\n        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n                or out_lemma.endswith('.'):\n            out_lemma = ''.join(out_lemma[:-1])\n    return out_lemma\n\n\ndef list_replace(search, replacement, text):\n    search = [el for el in search if el in text]\n    for c in search:\n        text = text.replace(c, replacement)\n    return text\n\n\ndef unify_sym(text):  # принимает строку в юникоде\n    text = list_replace \\\n        ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n\n    text = list_replace \\\n        ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n\n    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n\n    text = list_replace \\\n            (\n            '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n            '\\u2002', text)\n\n    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n    text = re.sub('\\t\\t', '\\t', text)\n\n    text = list_replace \\\n            (\n            '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n            '.', text)\n\n    text = list_replace('\\u2217', '\\u002A', text)\n\n    text = list_replace('…', '...', text)\n\n    text = list_replace('\\u2241\\u224B\\u2E2F\\u0483', '\\u223D', text)\n\n    text = list_replace('\\u00C4', 'A', text)  # латинская\n    text = list_replace('\\u00E4', 'a', text)\n    text = list_replace('\\u00CB', 'E', text)\n    text = list_replace('\\u00EB', 'e', text)\n    text = list_replace('\\u1E26', 'H', text)\n    text = list_replace('\\u1E27', 'h', text)\n    text = list_replace('\\u00CF', 'I', text)\n    text = list_replace('\\u00EF', 'i', text)\n    text = list_replace('\\u00D6', 'O', text)\n    text = list_replace('\\u00F6', 'o', text)\n    text = list_replace('\\u00DC', 'U', text)\n    text = list_replace('\\u00FC', 'u', text)\n    text = list_replace('\\u0178', 'Y', text)\n    text = list_replace('\\u00FF', 'y', text)\n    text = list_replace('\\u00DF', 's', text)\n    text = list_replace('\\u1E9E', 'S', text)\n\n    currencies = list \\\n            (\n            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n        )\n\n    alphabet = list \\\n            (\n            '\\t\\n\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ,.[]{}()=+-−*&^%$#@!?~;:0123456789§/\\|\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n\n    alphabet.append(\"'\")\n\n    allowed = set(currencies + alphabet)\n\n    cleaned_text = [sym for sym in text if sym in allowed]\n    cleaned_text = ''.join(cleaned_text)\n\n    return cleaned_text","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Предобработка текста"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n    # Если частеречные тэги не нужны (например, их нет в модели), выставьте pos=False\n    # в этом случае на выход будут поданы только леммы\n    # По умолчанию знаки пунктуации вырезаются. Чтобы сохранить их, выставьте punct=True\n\n    entities = {'PROPN'}\n    named = False\n    memory = []\n    mem_case = None\n    mem_number = None\n    tagged_propn = []\n\n    # обрабатываем текст, получаем результат в формате conllu:\n    processed = pipeline.process(text)\n\n    # пропускаем строки со служебной информацией:\n    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n\n    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n    tagged = [w.split('\\t') for w in content if w]\n\n    for t in tagged:\n        if len(t) != 10:\n            continue\n        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n        token = clean_token(token, misc)\n        lemma = clean_lemma(lemma, pos)\n        if not lemma or not token:\n            continue\n        if pos in entities:\n            if '|' not in feats:\n                tagged_propn.append('%s_%s' % (lemma, pos))\n                continue\n            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n            if 'Case' not in morph or 'Number' not in morph:\n                tagged_propn.append('%s_%s' % (lemma, pos))\n                continue\n            if not named:\n                named = True\n                mem_case = morph['Case']\n                mem_number = morph['Number']\n            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n                memory.append(lemma)\n                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n                    named = False\n                    past_lemma = '::'.join(memory)\n                    memory = []\n                    tagged_propn.append(past_lemma + '_PROPN')\n            else:\n                named = False\n                past_lemma = '::'.join(memory)\n                memory = []\n                tagged_propn.append(past_lemma + '_PROPN')\n                tagged_propn.append('%s_%s' % (lemma, pos))\n        else:\n            if not named:\n                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n                    lemma = num_replace(token)\n                tagged_propn.append('%s_%s' % (lemma, pos))\n            else:\n                named = False\n                past_lemma = '::'.join(memory)\n                memory = []\n                tagged_propn.append(past_lemma + '_PROPN')\n                tagged_propn.append('%s_%s' % (lemma, pos))\n\n    if not keep_punct:\n        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n    if not keep_pos:\n        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n    return tagged_propn","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Входные параметры"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {'ruscorpora_upos_cbow_300_20_2019': '180',\n          'ruwikiruscorpora_upos_skipgram_300_2_2019': '182',\n          'tayga_upos_skipgram_300_2_2019': '185'}\n\n\ntext_path = '../input/text-file/text.txt'\nsequence_len = 5\nmodel_name = 'tayga_upos_skipgram_300_2_2019'","execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Загрузка UDPipe модели"},{"metadata":{"trusted":true},"cell_type":"code","source":"udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\nudpipe_filename = udpipe_model_url.split('/')[-1]\n\nif not os.path.isfile(udpipe_filename):\n    print('UDPipe model not found. Downloading...', file=sys.stderr)\n    wget.download(udpipe_model_url)\n\nprint('Loading the model...', file=sys.stderr)\nmodel = Model.load(udpipe_filename)\nprocess_pipeline = Pipeline(model,\n                            'tokenize',\n                            Pipeline.DEFAULT,\n                            Pipeline.DEFAULT,\n                            'conllu')","execution_count":28,"outputs":[{"output_type":"stream","text":"Loading the model...\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Считаем построчно текстовый файл и обрабатываем его. Этот текст токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Processing input...', file=sys.stderr)\n\ntext = []\ntry:\n    with open(text_path, 'r', encoding='utf8') as f:\n        for line in f:\n            output = process(process_pipeline, text=unify_sym(line.strip()))\n            text.append(output)\nfinally:\n    f.close()\n\ntext = [word for line in text for word in line]","execution_count":29,"outputs":[{"output_type":"stream","text":"Processing input...\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Загрузка word2vec модели"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_url = 'http://vectors.nlpl.eu/repository/11/' + models[model_name] + '.zip'\nm = wget.download(model_url)\nmodel_file = model_url.split('/')[-1]\nwith zipfile.ZipFile(model_file, 'r') as archive:\n    stream = archive.open('model.bin')\n    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Формирование векторов из векторов"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectors = []\nrandom_element = model['молодой_ADJ']\nfor word in text:\n    try:\n        word_embedding = model[word]\n        vectors.append(word_embedding)\n    except KeyError:\n        print(\"Слова '{}' нет в словаре\".format(word))\n        vectors.append(np.zeros_like(random_element))\nvectors = np.array(vectors)\n\nshape = vectors.shape\nn_vectors = int(np.ceil(shape[0] / sequence_len))\nvectors.resize((n_vectors, sequence_len, shape[1]),\n               refcheck=False)\nprint(vectors.shape)","execution_count":19,"outputs":[{"output_type":"stream","text":"Слова 'не_PART' нет в словаре\nСлова 'не_PART' нет в словаре\nСлова 'не_PART' нет в словаре\nСлова 'все_PRON' нет в словаре\nСлова 'как_SCONJ' нет в словаре\nСлова 'с_ADP' нет в словаре\nСлова 'увяданье_NOUN' нет в словаре\nСлова 'я_PRON' нет в словаре\nСлова 'не_PART' нет в словаре\nСлова 'быть_AUX' нет в словаре\n(5, 5, 300)\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}