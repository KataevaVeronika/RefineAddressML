{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import keras.backend as K\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT_CHUNK_SIZE = 64","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\ntokenizer.save_pretrained('.')","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"('./vocab.txt', './special_tokens_map.json', './added_tokens.json')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = open('../input/terrorism-data/positive.txt', mode='r')\npositive = tokenizer.tokenize(file.read())\nlen_positive = len(positive)\npositive = pd.DataFrame([tokenizer.decode(tokenizer.convert_tokens_to_ids(positive[i : i + TEXT_CHUNK_SIZE])) \n                         for i in range(0, len_positive, TEXT_CHUNK_SIZE)])\npositive[1] = 1\nfile.close()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = open('../input/terrorism-data/negative.txt', mode='r')\nnegative = tokenizer.tokenize(file.read())\nlen_negative = len(negative)\nnegative = pd.DataFrame([tokenizer.decode(tokenizer.convert_tokens_to_ids(negative[i : i + TEXT_CHUNK_SIZE])) \n                         for i in range(0, len_negative, TEXT_CHUNK_SIZE)])\nnegative[1] = 0\nnegative = negative.sample(n=positive.shape[0], random_state=17)\nfile.close()","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random undersampling\ntexts = pd.concat([negative, positive]).sample(frac=1, random_state=17).reset_index(drop=True)\ntexts[1].value_counts()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"1    4116\n0    4116\nName: 1, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Делим данные на training, valid, test\nX = texts[0]\ny = texts[1]\nx_train_raw, x_test_raw, y_train, y_test = train_test_split(X,\n                                                            y,\n                                                            test_size=0.3,\n                                                            random_state=17)\nx_valid_raw, x_test_raw, y_valid, y_test = train_test_split(x_test_raw,\n                                                            y_test,\n                                                            test_size=0.33,\n                                                            random_state=17)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":9,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# hyperparameters\nEPOCHS = 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 128\nLEARNING_RATE = 3e-5","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Загрузка BertWordPieceTokenizer-а\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=False, wordpieces_prefix=##)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i : i + chunk_size].tolist()\n        encodings = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([encoding.ids for encoding in encodings])\n    \n    return np.array(all_ids)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_encoded = fast_encode(x_train_raw, fast_tokenizer, maxlen=MAX_LEN)\nx_valid_encoded = fast_encode(x_valid_raw, fast_tokenizer, maxlen=MAX_LEN)\n# x_test_encoded = fast_encode(x_test_raw, fast_tokenizer, maxlen=MAX_LEN)","execution_count":13,"outputs":[{"output_type":"stream","text":"100%|██████████| 23/23 [00:00<00:00, 33.92it/s]\n100%|██████████| 7/7 [00:00<00:00, 36.83it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (tf.data.Dataset\n                   .from_tensor_slices((x_train_encoded, y_train))\n                   .repeat()\n                   .shuffle(17)\n                   .batch(BATCH_SIZE)\n                   .prefetch(AUTO))\n\nvalid_dataset = (tf.data.Dataset\n                   .from_tensor_slices((x_valid_encoded, y_valid))\n                   .batch(BATCH_SIZE)\n                   .cache()\n                   .prefetch(AUTO))\n\n# test_dataset = (tf.data.Dataset\n#                   .from_tensor_slices(x_test_encoded)\n#                   .batch(BATCH_SIZE))","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# F1-score\ndef f1_score(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    return 2 * precision * recall / (precision + recall + K.epsilon())","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_terrorism_model(transformer, max_len=512):\n    \"\"\"\n    Function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy',\n                                                                               Precision(),\n                                                                               Recall(),\n                                                                               f1_score])\n    \n    return model","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (transformers.TFDistilBertModel\n                                     .from_pretrained('distilbert-base-multilingual-cased'))\n    terrorism_model = build_terrorism_model(transformer_layer, max_len=MAX_LEN)\nterrorism_model.summary()","execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1cb9a3c54ed409b9fd3aea1a5f018e3"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=910749124.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12497954bbe14d3b877a07b66f2ba356"}},"metadata":{}},{"output_type":"stream","text":"\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 128)]             0         \n_________________________________________________________________\ntf_distil_bert_model (TFDist ((None, 128, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice (T [(None, 768)]             0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 34.9 s, sys: 11.5 s, total: 46.3 s\nWall time: 1min 3s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_train_encoded.shape[0] // BATCH_SIZE\n\ntrain_history = terrorism_model.fit(train_dataset,\n                                    steps_per_epoch=n_steps,\n                                    validation_data=valid_dataset,\n                                    epochs=EPOCHS)","execution_count":18,"outputs":[{"output_type":"stream","text":"Epoch 1/4\n45/45 [==============================] - 14s 322ms/step - f1_score: 0.7223 - accuracy: 0.7748 - precision: 0.7877 - loss: 0.4455 - recall: 0.7548 - val_f1_score: 0.9003 - val_accuracy: 0.9015 - val_precision: 0.8603 - val_loss: 0.2358 - val_recall: 0.9575\nEpoch 2/4\n45/45 [==============================] - 5s 108ms/step - f1_score: 0.9265 - accuracy: 0.9314 - precision: 0.9425 - loss: 0.1742 - recall: 0.9193 - val_f1_score: 0.9175 - val_accuracy: 0.9190 - val_precision: 0.8773 - val_loss: 0.1972 - val_recall: 0.9733\nEpoch 3/4\n45/45 [==============================] - 5s 100ms/step - f1_score: 0.9485 - accuracy: 0.9521 - precision: 0.9615 - loss: 0.1209 - recall: 0.9422 - val_f1_score: 0.9423 - val_accuracy: 0.9492 - val_precision: 0.9613 - val_loss: 0.1437 - val_recall: 0.9356\nEpoch 4/4\n45/45 [==============================] - 4s 100ms/step - f1_score: 0.9635 - accuracy: 0.9661 - precision: 0.9710 - loss: 0.0855 - recall: 0.9613 - val_f1_score: 0.9352 - val_accuracy: 0.9432 - val_precision: 0.9703 - val_loss: 0.1707 - val_recall: 0.9137\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred = model.predict(test_dataset)\n# tf.math.confusion_matrix(y_test.tolist(),\n#                          y_pred.round().tolist(),\n#                          num_classes=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_sentiment_model(transformer, max_len=512):\n    \"\"\"\n    Function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=3e-5), loss='binary_crossentropy', metrics=['accuracy',\n                                                                               Precision(),\n                                                                               Recall(),\n                                                                               f1_score])\n    \n    return model\n\n\nwith strategy.scope():\n    transformer_layer = (transformers.TFDistilBertModel\n                                     .from_pretrained('distilbert-base-multilingual-cased'))\n    sentiment_model = build_sentiment_model(transformer_layer, max_len=189)\nsentiment_model.summary()","execution_count":19,"outputs":[{"output_type":"stream","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 189)]             0         \n_________________________________________________________________\ntf_distil_bert_model_1 (TFDi ((None, 189, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice_1  [(None, 768)]             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_model.load_weights('../input/bert-weights/model_weights.h5')","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classify(x_test_raw):\n    x_test_encoded = fast_encode(x_test_raw, fast_tokenizer, maxlen=MAX_LEN)\n    test_dataset = (tf.data.Dataset\n                      .from_tensor_slices(x_test_encoded)\n                      .batch(BATCH_SIZE))\n    terrorism_classes = pd.Series(terrorism_model.predict(test_dataset).round().ravel())\n    sentiment_classes = pd.Series(sentiment_model.predict(test_dataset).round().ravel())\n    classifications = pd.concat([terrorism_classes, sentiment_classes], axis=1)\n    classifications[0] = classifications[0].map({0: 'not_terrorism',\n                                                 1: 'terrorism'})\n    classifications[1] = classifications[1].map({0: 'negative',\n                                                 1: 'positive'})\n    classifications.to_csv('./classifications.csv', sep=';', header=None)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classify(x_test_raw)","execution_count":43,"outputs":[{"output_type":"stream","text":"100%|██████████| 4/4 [00:00<00:00, 42.60it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'classifications.csv')","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"/kaggle/working/classifications.csv","text/html":"<a href='classifications.csv' target='_blank'>classifications.csv</a><br>"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}