{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import keras.backend as K\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT_CHUNK_SIZE = 64","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\ntokenizer.save_pretrained('.')","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"('./vocab.txt', './special_tokens_map.json', './added_tokens.json')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = open('../input/terrorism-data/positive.txt', mode='r')\npositive = tokenizer.tokenize(file.read())\nlen_positive = len(positive)\npositive = pd.DataFrame([tokenizer.decode(tokenizer.convert_tokens_to_ids(positive[i : i + TEXT_CHUNK_SIZE])) \n                         for i in range(0, len_positive, TEXT_CHUNK_SIZE)])\npositive[1] = 1\nfile.close()","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = open('../input/terrorism-data/negative.txt', mode='r')\nnegative = tokenizer.tokenize(file.read())\nlen_negative = len(negative)\nnegative = pd.DataFrame([tokenizer.decode(tokenizer.convert_tokens_to_ids(negative[i : i + TEXT_CHUNK_SIZE])) \n                         for i in range(0, len_negative, TEXT_CHUNK_SIZE)])\nnegative[1] = 0\nnegative = negative.sample(n=positive.shape[0], random_state=17)\nfile.close()","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random undersampling\ntexts = pd.concat([negative, positive]).sample(frac=1, random_state=17).reset_index(drop=True)\ntexts[1].value_counts()","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"1    4116\n0    4116\nName: 1, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Делим данные на training, valid, test\nX = texts[0]\ny = texts[1]\nx_train_raw, x_test_raw, y_train, y_test = train_test_split(X,\n                                                            y,\n                                                            test_size=0.3,\n                                                            random_state=17)\nx_valid_raw, x_test_raw, y_valid, y_test = train_test_split(x_test_raw,\n                                                            y_test,\n                                                            test_size=0.33,\n                                                            random_state=17)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":29,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# hyperparameters\nEPOCHS = 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 128\nLEARNING_RATE = 3e-5","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Загрузка BertWordPieceTokenizer-а\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=False, wordpieces_prefix=##)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i : i + chunk_size].tolist()\n        encodings = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([encoding.ids for encoding in encodings])\n    \n    return np.array(all_ids)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_encoded = fast_encode(x_train_raw, fast_tokenizer, maxlen=MAX_LEN)\nx_valid_encoded = fast_encode(x_valid_raw, fast_tokenizer, maxlen=MAX_LEN)\n# x_test_encoded = fast_encode(x_test_raw, fast_tokenizer, maxlen=MAX_LEN)","execution_count":33,"outputs":[{"output_type":"stream","text":"100%|██████████| 23/23 [00:00<00:00, 45.64it/s]\n100%|██████████| 7/7 [00:00<00:00, 48.76it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (tf.data.Dataset\n                   .from_tensor_slices((x_train_encoded, y_train))\n                   .repeat()\n                   .shuffle(17)\n                   .batch(BATCH_SIZE)\n                   .prefetch(AUTO))\n\nvalid_dataset = (tf.data.Dataset\n                   .from_tensor_slices((x_valid_encoded, y_valid))\n                   .batch(BATCH_SIZE)\n                   .cache()\n                   .prefetch(AUTO))\n\n# test_dataset = (tf.data.Dataset\n#                   .from_tensor_slices(x_test_encoded)\n#                   .batch(BATCH_SIZE))","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# F1-score\ndef f1_score(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    return 2 * precision * recall / (precision + recall + K.epsilon())","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_terrorism_model(transformer, max_len=512):\n    \"\"\"\n    Function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=LEARNING_RATE), loss='binary_crossentropy')\n    \n    return model","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (transformers.TFDistilBertModel\n                                     .from_pretrained('distilbert-base-multilingual-cased'))\n    terrorism_model = build_terrorism_model(transformer_layer, max_len=MAX_LEN)\nterrorism_model.summary()","execution_count":37,"outputs":[{"output_type":"stream","text":"Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 128)]             0         \n_________________________________________________________________\ntf_distil_bert_model_2 (TFDi ((None, 128, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice_2  [(None, 768)]             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 5.41 s, sys: 2.61 s, total: 8.01 s\nWall time: 11.3 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_train_encoded.shape[0] // BATCH_SIZE\n\ntrain_history = terrorism_model.fit(train_dataset,\n                                    steps_per_epoch=n_steps,\n                                    validation_data=valid_dataset,\n                                    epochs=EPOCHS)","execution_count":38,"outputs":[{"output_type":"stream","text":"Epoch 1/4\n45/45 [==============================] - 13s 290ms/step - loss: 0.4676 - val_loss: 0.2142\nEpoch 2/4\n45/45 [==============================] - 4s 84ms/step - loss: 0.1842 - val_loss: 0.1857\nEpoch 3/4\n45/45 [==============================] - 4s 84ms/step - loss: 0.1051 - val_loss: 0.2477\nEpoch 4/4\n45/45 [==============================] - 4s 93ms/step - loss: 0.0814 - val_loss: 0.1689\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred = model.predict(test_dataset)\n# tf.math.confusion_matrix(y_test.tolist(),\n#                          y_pred.round().tolist(),\n#                          num_classes=2)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_sentiment_model(transformer, max_len=512):\n    \"\"\"\n    Function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=3e-5), loss='binary_crossentropy')\n    return model\n\n\nwith strategy.scope():\n    transformer_layer = (transformers.TFDistilBertModel\n                                     .from_pretrained('distilbert-base-multilingual-cased'))\n    sentiment_model = build_sentiment_model(transformer_layer, max_len=189)\nsentiment_model.summary()","execution_count":40,"outputs":[{"output_type":"stream","text":"Model: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 189)]             0         \n_________________________________________________________________\ntf_distil_bert_model_3 (TFDi ((None, 189, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice_3  [(None, 768)]             0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_model.load_weights('../input/bert-weights/model_weights.h5')","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classify(x_test_raw):\n    x_test_encoded = fast_encode(x_test_raw, fast_tokenizer, maxlen=MAX_LEN)\n    test_dataset = (tf.data.Dataset\n                      .from_tensor_slices(x_test_encoded)\n                      .batch(BATCH_SIZE))\n    terrorism_predictions = terrorism_model.predict(test_dataset).round()\n    \n    print(classification_report(y_test, terrorism_predictions, target_names=['positive', 'negative']))\n    print(tf.math.confusion_matrix(y_test.tolist(),\n                                   terrorism_predictions.round().tolist(),\n                                   num_classes=2))\n   \n    sentiment_classes = pd.Series(sentiment_model.predict(test_dataset).round().ravel())\n    classifications = pd.concat([pd.Series(terrorism_predictions.ravel()), sentiment_classes], axis=1)\n    classifications[0] = classifications[0].map({0: 'not_terrorism',\n                                                 1: 'terrorism'})\n    classifications[1] = classifications[1].map({0: 'negative',\n                                                 1: 'positive'})\n    classifications.to_csv('./classifications.csv', sep=';', header=None)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classify(x_test_raw)","execution_count":43,"outputs":[{"output_type":"stream","text":"100%|██████████| 4/4 [00:00<00:00, 37.06it/s]\n","name":"stderr"},{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n    positive       0.93      0.95      0.94       415\n    negative       0.94      0.93      0.94       401\n\n    accuracy                           0.94       816\n   macro avg       0.94      0.94      0.94       816\nweighted avg       0.94      0.94      0.94       816\n\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'y_pred' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-9765a179f027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-42-fbfa87901d89>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(x_test_raw)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterrorism_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     print(tf.math.confusion_matrix(y_test.tolist(),\n\u001b[0;32m---> 10\u001b[0;31m                                    \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                                    num_classes=2))\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'classifications.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}