{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tokenizers import BertWordPieceTokenizer\nfrom tqdm import tqdm\nfrom os import listdir\nfrom os.path import join, isdir","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"classifiers_path = ''\nresults_path = ''\nto_predict_path = ''\ntext_chunk_size = 64\nepochs = 4\nbatch_size = 16\nmax_len = 128\nlearning_rate = 3e-5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classification_model(classifiers_path, to_predict_path, results_path):\n    \n    # restore model structure\n    def build_model(transformer, max_len=512):\n        \n        input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n        sequence_output = transformer(input_word_ids)[0]\n        cls_token = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(cls_token)\n\n        model = Model(inputs=input_word_ids, outputs=out)\n        model.compile(Adam(lr=learning_rate), loss='binary_crossentropy')\n        return model\n\n    \n    # encoder for encoding the text into sequence of integers for BERT Input\n    def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n\n        tokenizer.enable_truncation(max_length=maxlen)\n        tokenizer.enable_padding(max_length=maxlen)\n        all_ids = []\n\n        for i in tqdm(range(0, len(texts), chunk_size)):\n            text_chunk = texts[i : i + chunk_size].tolist()\n            encodings = tokenizer.encode_batch(text_chunk)\n            all_ids.extend([encoding.ids for encoding in encodings])\n\n        return np.array(all_ids)\n    \n\n    # TPU\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n        \n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n\n        strategy = tf.distribute.get_strategy()\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n\n    # importing tokenizer\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n    tokenizer.save_pretrained('.')\n\n    # get all directories\n    directories = [directory for directory in listdir(classifiers_path) \n                   if isdir(join(classifiers_path, directory))]\n    \n    # import BertWordPieceTokenizer\n    fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n    \n    # import models\n    with strategy.scope():\n        transformer_layer = (transformers.TFDistilBertModel\n                                         .from_pretrained('distilbert-base-multilingual-cased'))\n\n        classifications = pd.DataFrame([])\n        \n        for directory in directories:\n            parameters = {}\n            with open(classifiers_path + directory + '/model.properties', 'r') as file:\n                lines = file.readlines()\n                for line in lines:\n                    parameter = line.strip('\\n').split('=')\n                    parameters[parameter[0]] = parameter[1]\n            \n            model = build_model(transformer_layer,\n                                int(parameters['max_len']))\n            \n            model.load_weights(classifiers_path + directory + '/model_weights.h5')\n            print(model.summary())\n            \n            # import dataset\n            x_test_raw = pd.read_csv(to_predict_path, sep=';', index_col=0, header=None)\n            x_test_encoded = fast_encode(x_test_raw.iloc[:, 0], fast_tokenizer, maxlen=int(parameters['max_len']))\n            test_dataset = (tf.data.Dataset\n                                   .from_tensor_slices(x_test_encoded)\n                                   .batch(int(parameters['batch_size']) * strategy.num_replicas_in_sync))\n            # predictions\n            predictions = model.predict(test_dataset).round()\n\n            # get labels\n            labels = parameters['labels'].split(',')\n            labels = {index: labels[index] for index in range(len(labels))}\n\n            # add and map predictions\n            classifications = pd.concat([classifications, pd.Series(predictions.ravel()).map(labels)], axis=1)\n    classifications.to_csv(results_path + 'classifications.csv', sep=';', header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_model(classifiers_path, to_predict_path, results_path)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}