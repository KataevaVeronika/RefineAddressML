{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report\nfrom os import listdir\nfrom os.path import isfile, join","execution_count":2,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"input_path = '../input/terrorism-data/'\noutput_path = './'\ntext_chunk_size = 64\nepochs = 4\nbatch_size = 16\nmax_len = 128\nlearning_rate = 3e-5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(input_path, output_path, text_chunk_size, epochs, batch_size, max_len, learning_rate):\n    \n    # encoder for encoding the text into sequence of integers for BERT Input\n    def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n\n        tokenizer.enable_truncation(max_length=maxlen)\n        tokenizer.enable_padding(max_length=maxlen)\n        all_ids = []\n\n        for i in tqdm(range(0, len(texts), chunk_size)):\n            text_chunk = texts[i : i + chunk_size].tolist()\n            encodings = tokenizer.encode_batch(text_chunk)\n            all_ids.extend([encoding.ids for encoding in encodings])\n\n        return np.array(all_ids)\n    \n    \n    # function for training the BERT model\n    def build_model(transformer, max_len=512):\n        input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n        sequence_output = transformer(input_word_ids)[0]\n        cls_token = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(cls_token)\n\n        model = Model(inputs=input_word_ids, outputs=out)\n        model.compile(Adam(lr=learning_rate), loss='binary_crossentropy')\n\n        return model\n    \n    \n    # TPU\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n        \n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n\n        strategy = tf.distribute.get_strategy()\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    \n    # batch size according to TPU\n    batch_size *= strategy.num_replicas_in_sync\n    \n    # importing tokenizer\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n    tokenizer.save_pretrained('.')\n    \n    # importing files and creating dataframe\n    files = [file for file in listdir(input_path) if isfile(join(input_path, file))]\n    files.sort()\n    \n    df = pd.DataFrame([])\n    label = 0\n    classes = [file.split('.')[0] for file in files]\n\n    for file in files:\n        file = open(input_path + file, mode='r')\n        label_data = tokenizer.tokenize(file.read())\n        len_label_data = len(label_data)\n        label_df = pd.DataFrame([tokenizer.decode(tokenizer.convert_tokens_to_ids(label_data[i : i + text_chunk_size])) \n                                 for i in range(0, len_label_data, text_chunk_size)])\n        label_df[1] = label\n        df = pd.concat([df, label_df], ignore_index=True)\n        label += 1\n        file.close()\n    \n    # random undersampling\n    min_class_n = df[1].value_counts().min()\n    df = df.groupby(1).apply(lambda x: x.sample(min_class_n)).reset_index(drop=True)\n    \n    # split data\n    X = df[0]\n    y = df[1]\n    x_train_raw, x_test_raw, y_train, y_test = train_test_split(X,\n                                                                y,\n                                                                test_size=0.3,\n                                                                random_state=17)\n    x_valid_raw, x_test_raw, y_valid, y_test = train_test_split(x_test_raw,\n                                                                y_test,\n                                                                test_size=0.33,\n                                                                random_state=17)\n   \n    # import BertWordPieceTokenizer\n    fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n    \n    # dataset encoding\n    x_train_encoded = fast_encode(x_train_raw, fast_tokenizer, maxlen=max_len)\n    x_valid_encoded = fast_encode(x_valid_raw, fast_tokenizer, maxlen=max_len)\n    x_test_encoded = fast_encode(x_test_raw, fast_tokenizer, maxlen=max_len)\n    \n    \n    train_dataset = (tf.data.Dataset\n                       .from_tensor_slices((x_train_encoded, y_train))\n                       .repeat()\n                       .shuffle(17)\n                       .batch(batch_size)\n                       .prefetch(AUTO))\n\n    valid_dataset = (tf.data.Dataset\n                       .from_tensor_slices((x_valid_encoded, y_valid))\n                       .batch(batch_size)\n                       .cache()\n                       .prefetch(AUTO))\n\n    test_dataset = (tf.data.Dataset\n                      .from_tensor_slices(x_test_encoded)\n                      .batch(batch_size))\n    \n    with strategy.scope():\n        transformer_layer = (transformers.TFDistilBertModel\n                                         .from_pretrained('distilbert-base-multilingual-cased'))\n        model = build_model(transformer_layer, max_len)\n    print(model.summary())\n    \n    n_steps = x_train_encoded.shape[0] // batch_size\n\n    train_history = model.fit(train_dataset,\n                              steps_per_epoch=n_steps,\n                              validation_data=valid_dataset,\n                              epochs=epochs)\n    print(train_history)\n    predictions = model.predict(test_dataset).round()\n    \n    # assessments\n    assessments = open(output_path + 'assessments.txt', 'w')\n    assessments.write(classification_report(y_test,\n                                            predictions,\n                                            target_names=classes) + \\\n                                            '\\nConfusion Matrix\\n' + \\\n                      str(tf.math.confusion_matrix(y_test.tolist(),\n                          predictions.round().tolist(),\n                          num_classes=len(classes)).numpy()))\n    assessments.close()\n\n    # save model's hyperparameters\n    parameters = open(output_path + 'model.properties', 'w')\n    parameters.write(f'text_chunk_size={text_chunk_size}\\n' \\\n                     f'epochs={epochs}\\n' \\\n                     f'batch_size={batch_size // strategy.num_replicas_in_sync}\\n' \\\n                     f'max_len={max_len}\\n' \\\n                     f'learning_rate={learning_rate}\\n' \\\n                     f'labels={\",\".join(classes)}')\n    parameters.close()\n    \n    # save model's weights\n    model.save_weights(output_path + 'model_weights.h5')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(input_path, output_path, text_chunk_size, epochs, batch_size, max_len, learning_rate)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}